\chapter{Algorithms}
\label{appendix:algorithms}
This appendix contains an example algorithm.
\begingroup\emergencystretch=3em
\begin{sloppypar}
\setlength{\textfloatsep}{6pt}

\newcommand{\hh}{\hspace*{3pt}}

\SetKwInput{KwInit}{Init}
\SetKwInput{KwHyper}{Hyperparameters}

\begin{algorithm}[H]
\DontPrintSemicolon
\caption{Principal Component Analysis (PCA)}
\label{alg:pca}

\KwIn{Dataset $\mathbf{X} \in \mathbb{R}^{n \times d}$ (n samples, d features)}
\KwOut{Reduced representation $\mathbf{X}_{\text{reduced}}$}

\Begin{
    {\begingroup\scriptsize\sloppy\raggedright\setlength{\emergencystretch}{3em}
        Center the data: $\mathbf{X} \leftarrow \mathbf{X} - \text{mean}(\mathbf{X})$\;
        Compute covariance matrix: $\mathbf{C} = \frac{1}{n-1} \mathbf{X}^T \mathbf{X}$\;
        Compute eigenvectors and eigenvalues of $\mathbf{C}$\;
        Sort eigenvectors by descending eigenvalues\;
        Select top $k$ eigenvectors to form matrix $\mathbf{W}$\;
        Project data: $\mathbf{X}_{\text{reduced}} = \mathbf{X} \mathbf{W}$
    \endgroup}
}
\end{algorithm}


\begin{algorithm}[H]
\DontPrintSemicolon
\caption{Autoencoder Training for Feature Extraction}
\label{alg:autoencoder}

\KwIn{Training data $\mathbf{X}$}
\KwOut{Encoded representation $\mathbf{Z}$}

\Begin{
    {\begingroup\scriptsize\sloppy\raggedright\setlength{\emergencystretch}{3em}
        Define encoder network $f_{\theta}$ and decoder network $g_{\phi}$\;
        Initialize parameters $\theta, \phi$\;
        \For{each epoch}{
                \For{each mini-batch}{
                        Encode: $\mathbf{Z} = f_{\theta}(\mathbf{X})$\;
                        Decode: $\hat{\mathbf{X}} = g_{\phi}(\mathbf{Z})$\;
                        Compute reconstruction loss: $L = \|\mathbf{X} - \hat{\mathbf{X}}\|^2$\;
                        Update $\theta, \phi$ using gradient descent to minimize $L$\;
                }
        }
    \endgroup}
}
\end{algorithm}
\begin{algorithm}[H]
\DontPrintSemicolon
\caption{Neural Collaborative Filtering (NCF)}
\label{alg:ncf-appendix}

\KwIn{User-item interaction matrix $\mathbf{R}$}
\KwOut{Predicted user preferences $\hat{\mathbf{R}}$}

\Begin{
    {\begingroup\scriptsize\sloppy\raggedright\setlength{\emergencystretch}{3em}
        Initialize embeddings for users and items\;
        Define neural network $f(\cdot)$ to model interaction between embeddings\;
        \For{each epoch}{
                \For{each user-item pair $(u, i)$}{
                        Predict score: $\hat{r}_{ui} = f(\text{embedding}(u), \text{embedding}(i))$\;
                        Compute loss: $L = \text{MSE}(r_{ui}, \hat{r}_{ui})$\;
                        Update model parameters via backpropagation\;
                }
        }
    \endgroup}
}
\end{algorithm}
\begin{algorithm}[H]
\DontPrintSemicolon
\caption{Graph Neural Network (GNN) for Skill Recommendation}
\label{alg:gnn-appendix}

\KwIn{Graph $G = (V, E)$ with node features $\mathbf{H}^0$}
\KwOut{Updated node embeddings $\mathbf{H}^L$}

\Begin{
    {\begingroup\scriptsize\sloppy\raggedright\setlength{\emergencystretch}{3em}
        \For{layer $l = 0$ \KwTo $L-1$}{
                \For{each node $v \in V$}{
                        Aggregate neighbor features:
            
                        $\mathbf{h}_{\mathcal{N}(v)}^{(l)} = \text{AGGREGATE}^{(l)}(\{\mathbf{h}_u^{(l)}: u \in \mathcal{N}(v)\})$\;
            
                        Update node representation:
            
                        $\mathbf{h}_v^{(l+1)} = \sigma\left( \mathbf{W}^{(l)} \cdot \text{CONCAT}(\mathbf{h}_v^{(l)}, \mathbf{h}_{\mathcal{N}(v)}^{(l)}) \right)$\;
                }
        }
    \endgroup}
}
\end{algorithm}
\begin{algorithm}[H]
\DontPrintSemicolon
\caption{Transformer Encoder for Text Embeddings}
\label{alg:transformer-appendix}

\KwIn{Tokenized text input $X = (x_1, \ldots, x_n)$}
\KwOut{Contextual embeddings $H = (h_1, \ldots, h_n)$}

\Begin{
    {\begingroup\scriptsize\sloppy\raggedright\setlength{\emergencystretch}{3em}
        Embed tokens: $\mathbf{E} = \text{Embedding}(X)$\;
        Add positional encodings to $\mathbf{E}$\;
        \For{each layer}{
                Compute multi-head self-attention:

                $Z = \text{MultiHead}(Q=E, K=E, V=E)$\;
        
                Apply feedforward network: 
        
                $H = \text{FFN}(Z)$\;
        }
    \endgroup}
}
\end{algorithm}
\begin{algorithm}[H]
\DontPrintSemicolon
\caption{Deep Learning-based IDP Recommender System Pipeline}
\label{alg:idp_pipeline}

\KwIn{Raw employee data: skills assessments, job descriptions, performance reviews, career goals}
\KwOut{Personalized Individual Development Plans (IDPs)}

\Begin{
    {\begingroup\scriptsize\sloppy\raggedright\setlength{\emergencystretch}{3em}
    	extbf{Step 1: Data Collection and Preprocessing}\;
        \Indp
        Collect employee datasets (skills, evaluations, feedback, goals)\;
        Clean and normalize data (handling missing values, text cleaning)\;
        Extract features from structured data and unstructured text using NLP\;
        \Indm

             	extbf{Step 2: Feature Engineering}\;
        \Indp
        Apply PCA for dimensionality reduction\;
        Train Autoencoder to learn compact feature representations\;
        Perform Clustering to identify similar employee profiles\;
        \Indm

             	extbf{Step 3: Model Training}\;
        \Indp
             	extbf{Sub-step 3.1: NCF-based Interaction Modeling}\;
        \Indp
        Train Neural Collaborative Filtering (NCF) to predict personalized recommendations based on historical data\;
        \Indm
             	extbf{Sub-step 3.2: Graph Modeling}\;
        \Indp
        Construct Knowledge Graph of skills, learning paths, career objectives\;
        Train Graph Neural Network (GNN) to capture relational dependencies\;
        \Indm
             	extbf{Sub-step 3.3: Textual Understanding}\;
        \Indp
        Train Transformer model (e.g., BERT) on employee goal statements\;
        Generate embeddings for career goal descriptions\;
        \Indm
        \Indm

             	extbf{Step 4: Inference and Recommendation}\;
        \Indp
        For a given employee:
        \begin{itemize}
                \item Embed structured and unstructured data
                \item Aggregate predictions from NCF, GNN, and Transformer models
                \item Rank and select top development plan recommendations
        \end{itemize}
        Generate customized IDP output for the employee\;
        \Indm

             	extbf{Step 5: Feedback Integration}\;
        \Indp
        Collect employee and supervisor feedback post-recommendation\;
        Update training datasets and retrain models periodically for dynamic adaptation\;
        \Indm
    \endgroup}
}
\end{algorithm}
\end{sloppypar}
\fussy
\endgroup
